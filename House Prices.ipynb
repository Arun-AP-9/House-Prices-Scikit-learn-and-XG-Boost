{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load the train dataset\n\ndf_train = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\",index_col='Id')\n\n#see the head of the data\n\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Well, a lot of columns and we can see a lot of null values as well\n\n# the data description also indicates a number of categories\n\n# some categories maybe better stored as numericals ? 0-poor to 4-excellent\n\n# let us first check the number of null values and see what columns we can realistically use\n\n# we have 1460 rows of data\n\nnull_columns=df_train.columns[df_train.isnull().any()]\n\ndf_train[null_columns].isnull().sum()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Using a simple imputer for missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_knn = df_train\n\ndf_train_knn.info(verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_knn[['SalePrice']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert all objects into categorical data types\n\n# use a for loop to convert all of these columns into categoris\n\ndf_train_2 = df_train_knn.select_dtypes(include='object')\n\ncolumns_obj_test= df_train_2.columns\n\nfor col in columns_obj_test:\n    df_train_knn[col] = df_train_knn[col].astype('category')\n    \n# let us see if this has worked\n\ndf_train_4 = df_train_knn.drop(columns=['Alley','MiscFeature','SalePrice'])\n\ndf_train_4.info()\n\n# yes, it has","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using Pipelines for Imputation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will use the KnnImputer for numeric imputations and Simple Imputer for categorical imputation\n\n# let us first bring all the necessary modules\n\n# guide on how to do this with categorical tranfromer as in https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.pipeline import make_pipeline\n\nfrom fancyimpute import KNN\n\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nnumeric_transformer = Pipeline(steps=[('imputer',SimpleImputer(missing_values=np.nan, strategy='mean') ),('scaler',StandardScaler())])\n\ncategorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Next we use the ColumnTransformer to apply the transformations to the correct columns in the dataframe. \n\n# Before building this we have stored lists of the numeric and categorical columns using the pandas dtype method\n\nnumeric_features = df_train_4.select_dtypes(include=['int64', 'float64']).columns\n\ncategorical_features = df_train_4.select_dtypes(include=['category']).columns \n\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)])\n\n\n#transformed = preprocessor.fit_transform(df_train_4)\n\n#df_train_clean = pd.DataFrame(data=transformed, index=df_train_4.index, columns=df_train_4.columns)\n\n#df_train_clean.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Wow, the amazing pipelines have done everything for us\n\n# let us double check to make sure there are no null columns any more\n\n#null_columns2=df_train_clean.columns[df_train_clean.isnull().any()]\n\n#df_train_clean[null_columns2].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow, that is all the null values dealt with and very effeciently"},{"metadata":{},"cell_type":"markdown","source":"# Using Pipelines with Scikit learn models"},{"metadata":{"trusted":true},"cell_type":"code","source":"# now while actually running a model we will run the pre-processing step above as just one part of a pipeline\n\n# first let us bring the features and labels\n\nX = df_train_4\n\ny = df_train[['SalePrice']]\n\n# let us import train test split\n\nfrom sklearn.model_selection import train_test_split\n\n# Split data into 70% train and 30% test\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size= 0.3,random_state= 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let us bring the scikit classifiers and also bring in XG boost\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nfrom sklearn.model_selection import cross_val_score\n\n# let us use PCA as well \n \n# In some cases dimensionality reduction may filter out some noice and unnecessary details and result in higher performance\n\n# but in general it won't, it will just speed up training\n\nfrom sklearn.decomposition import PCA\n\n# we need to import TruncatedSVD as PCA doesn't support sparse matrixes\n\nfrom sklearn.decomposition import TruncatedSVD\n\nSEED=1\n\nridge = Ridge(alpha=0.1)\nknn = KNeighborsRegressor()\ntree = DecisionTreeRegressor()\nrf = RandomForestRegressor()\nxgb= xgb.XGBRegressor()\n\n# Define a list called classifier that contains the tuples (classifier_name, classifier)\n\nclassifiers = [('Ridge Regression', ridge),('K Nearest Neighbours', knn),('Decision Tree', tree),('Random Forest', rf),('XGBoost', xgb)]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Iterate over the defined list of tuples containing the classifiers\n\nfor clf_name, clf in classifiers:\n    \n# create the full pipeline to the training set\n\n   pipe = Pipeline(steps=[('preprocessor', preprocessor),('reducer',TruncatedSVD(n_components=30)),('regressor',  clf)])\n\n   pipe.fit(X_train, np.ravel(y_train))\n    \n# Predict the labels of the test set\n\n   y_pred = pipe.predict(X_test)\n           \n# Evaluate accuracies using cross_val_score\n    \n   cv_scores_rmse = np.sqrt(-1*(cross_val_score(pipe,X,np.ravel(y),cv=5,scoring='neg_mean_squared_error').mean()))\n   cv_scores_r2 = cross_val_score(pipe,X,np.ravel(y),cv=5).mean()\n          \n\n# print the cv_scores for each classifier\n   print('{:s} : {:.3f}'.format(clf_name, cv_scores_rmse))\n   print('{:s} : {:.3f}'.format(clf_name,  cv_scores_r2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Without any tuning, our XGBoost algorithm is already our best"},{"metadata":{},"cell_type":"markdown","source":"Let us see how the PCA impacts our models and see how much the dimensionality reduction helps us "},{"metadata":{"trusted":true},"cell_type":"code","source":"# we are going to access step 2 which is at index 1\n\npipe.steps[1]\n\n# let us see how the components explain the variances\n\nvar_sum = pipe.steps[1][1].explained_variance_ratio_.cumsum()\n\nvar_sum","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let us plot this to find the elbow in the plot\n\nimport matplotlib.pyplot as plt\n\nvar = pipe.steps[1][1].explained_variance_ratio_\n\nplt.plot(var)\nplt.xlabel('Principal component index')\nplt.ylabel('Explained variance ratio')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" * Let us just work with the XG Boost model, first with random hyper parameteres chosen by us"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport xgboost as xgb\n\n\nxgb= xgb.XGBRegressor(colsample_bytree=0.3, learning_rate= 0.1, max_depth= 4)\n\nxgb_pipe1 = Pipeline(steps=[('preprocessor', preprocessor),('reducer',TruncatedSVD(n_components=30)),('regressor',  xgb)])\n\n# let us see if we can put in an RFE for feature selection\n\n\nxgb_pipe1.fit(X_train, np.ravel(y_train))\n    \n# Predict the labels of the test set\n\ny_pred = xgb_pipe1.predict(X_test)\n\n# Evaluate accuracies using cross_val_score\n    \ncv_scores_rmse = np.sqrt(-1*(cross_val_score(xgb_pipe1,X,np.ravel(y),cv=5,scoring='neg_mean_squared_error').mean()))\ncv_scores_r2 = cross_val_score(xgb_pipe1,X,np.ravel(y),cv=5).mean()\n    \n# print the cv_scores for each classifier\nprint('{:s} : {:.3f}'.format(clf_name, cv_scores_rmse))\nprint('{:s} : {:.3f}'.format(clf_name,  cv_scores_r2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"An improvement in performace to 1%\n\nLet us try to do Randomozed Search CV"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import random search cv\n\nimport xgboost as xgb\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn.model_selection import GridSearchCV\n\nimport time\n\nt_start=time.time()\n\nxgb= xgb.XGBRegressor(seed=123)\n\nxgb_pipe = Pipeline(steps=[('preprocessor', preprocessor),('reducer',TruncatedSVD(n_components=30)),('regressor',  xgb)])\n\ngbm_param_grid = {'regressor__learning_rate': np.arange(0.05,1.05,.05),'regressor__n_estimators': np.arange(50, 500, 50),'regressor__subsample': np.arange(0.05,1.05,.05),'regressor__max_depth' : np.arange(2, 6),'regressor__colsample_bytree': np.arange(0.05,1.05,.05)}\n\ngrid_mse = RandomizedSearchCV(estimator=xgb_pipe,param_distributions=gbm_param_grid,n_iter= 25, cv=4, verbose=1,n_jobs=-1)\n\n# Fit randomized_mse to the data\n\ngrid_mse.fit(X_train, np.ravel(y_train))\n\n# Print the best parameters and lowest RMSE\nprint(\"Best parameters found: \", grid_mse.best_params_)\n\nprint(\"Lowest R squared found: \", np.sqrt(np.abs(grid_mse.best_score_)))\n\nt_end=time.time()\n\nprint(t_end-t_start)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Trying some dask"},{"metadata":{"trusted":true},"cell_type":"code","source":"# let us see how the above work with dask hyperparamter method\n\nfrom dask_ml.model_selection import RandomizedSearchCV\n\nimport xgboost as xgb\n\nimport time\n\nt_start=time.time()\n\nxgb= xgb.XGBRegressor(seed=123)\n\nxgb_pipe = Pipeline(steps=[('preprocessor', preprocessor),('reducer',TruncatedSVD(n_components=30)),('regressor',  xgb)])\n\ngbm_param_grid = {'regressor__learning_rate': np.arange(0.05,1.05,.05),'regressor__n_estimators': np.arange(50, 500, 50),'regressor__subsample': np.arange(0.05,1.05,.05),'regressor__max_depth' : np.arange(2, 6),'regressor__colsample_bytree': np.arange(0.05,1.05,.05)}\n\ngrid_mse = RandomizedSearchCV(estimator=xgb_pipe,param_distributions=gbm_param_grid,n_iter= 50, cv=4, n_jobs=-1)\n\n# Fit randomized_mse to the data\n\ngrid_mse.fit(X_train, np.ravel(y_train))\n\n# Print the best parameters and lowest RMSE\nprint(\"Best parameters found: \", grid_mse.best_params_)\n\nprint(\"Lowest R squared found: \", np.sqrt(np.abs(grid_mse.best_score_)))\n\nt_end=time.time()\n\nprint(t_end-t_start)","execution_count":47,"outputs":[{"output_type":"stream","text":"Best parameters found:  {'regressor__subsample': 0.8, 'regressor__n_estimators': 100, 'regressor__max_depth': 3, 'regressor__learning_rate': 0.3, 'regressor__colsample_bytree': 0.7000000000000001}\nLowest R squared found:  0.9228162836303117\n42.00663757324219\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Dask Hyperparameter tuning was 20% faster! We need to try this on a larger data set though!\n\nWhy you use Dask Hyperparameter tuning, see reasons in https://ml.dask.org/hyper-parameter-search.html"},{"metadata":{},"cell_type":"markdown","source":"# Let's use Dask to distribute work across a cluster"},{"metadata":{"trusted":true},"cell_type":"code","source":"# let us see how the above work with dask hyperparamter method\n\nfrom dask_ml.model_selection import RandomizedSearchCV\n\n# let us import client to use my local machine as a cluster\n\nfrom dask.distributed import Client\n\n# we need joblib as well in backend\n\nimport joblib\n\nimport xgboost as xgb\n\nimport time\n\n#create local cluster\n\nclient = Client(processes=False)             \n\nt_start=time.time()\n\nxgb= xgb.XGBRegressor(seed=123)\n\nxgb_pipe = Pipeline(steps=[('preprocessor', preprocessor),('reducer',TruncatedSVD(n_components=30)),('regressor',  xgb)])\n\ngbm_param_grid = {'regressor__learning_rate': np.arange(0.05,1.05,.05),'regressor__n_estimators': np.arange(50, 500, 50),'regressor__subsample': np.arange(0.05,1.05,.05),'regressor__max_depth' : np.arange(2, 6),'regressor__colsample_bytree': np.arange(0.05,1.05,.05)}\n\ngrid_mse = RandomizedSearchCV(estimator=xgb_pipe,param_distributions=gbm_param_grid,n_iter= 50, cv=4, n_jobs=-1)\n\nwith joblib.parallel_backend('dask'):\n    \n# Fit randomized_mse to the data\n\n    grid_mse.fit(X_train, np.ravel(y_train))\n    \n    # Print the best parameters and lowest RMSE\n    print(\"Best parameters found: \", grid_mse.best_params_)\n\n    print(\"Lowest R squared found: \", np.sqrt(np.abs(grid_mse.best_score_)))\n    \n\nt_end=time.time()\n\nprint(t_end-t_start)","execution_count":48,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/distributed/dashboard/core.py:72: UserWarning: \nPort 8787 is already in use. \nPerhaps you already have a cluster running?\nHosting the diagnostics dashboard on a random port instead.\n  warnings.warn(\"\\n\" + msg)\n","name":"stderr"},{"output_type":"stream","text":"Best parameters found:  {'regressor__subsample': 0.05, 'regressor__n_estimators': 200, 'regressor__max_depth': 3, 'regressor__learning_rate': 0.05, 'regressor__colsample_bytree': 0.8}\nLowest R squared found:  0.9134398223065845\n22.721360206604004\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"*** Let us now work on the test data with the Pipeline architecture and make predictions**"},{"metadata":{},"cell_type":"markdown","source":"Now, we are talking with a 92.70 % R squared value"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the test dataset\n\ndf_test = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\",index_col='Id')\n\n#see the head of the data\n\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the info of the data\n\ndf_test.info()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = grid_mse.predict(df_test)\n\npred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let us put this into the submission format\n\nxgb_output=pd.DataFrame({'Id':df_test.index, 'SalePrice': pred}) \n\n# save to kaggle\n\nxgb_output.to_csv('my_submission_xgb.csv', index=False)\n\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Trying a Random Forest model with hyperparameter tuning"},{"metadata":{},"cell_type":"markdown","source":"Our untuned random forest gave us an R-squared of 0.828; can we improve on this?"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_log_error\n\n# instantiate the random forest regressor\n\nrf = RandomForestRegressor()\n\nrf_pipe = Pipeline(steps=[('preprocessor', preprocessor),('reducer',TruncatedSVD(n_components=30)),('regressor',  rf)])\n\n# set the important parameters for random forest\n\n# n_etimators will increase the number of trees built and bring in more diversity and a better model\n\n# max_features, decreasing this limit from one will make the number of features low at every tree and bring more diversity in each tree output or less correlated trees\n\n# n_jobs = -1 to use all CPU Cores (should be used within random search cv)\n\n# max_depth to be limited to restrict over fitting and for model to finish quickly\n\n# use random state to repeat the results\n\nrf_param_grid = {'regressor__n_estimators': np.arange(100,1000,100),'regressor__max_features': np.arange(0.05,1.05,.05),'regressor__max_depth':np.arange(4,8,1)}\n\ngrid_mse_rf = RandomizedSearchCV(estimator=rf_pipe,param_distributions=rf_param_grid,n_iter= 20, cv=4, verbose=1,n_jobs=-1, random_state=123)\n\n# Fit randomized_mse to the data\n\ngrid_mse_rf.fit(X_train, np.ravel(y_train))\n\n# Print the best parameters and lowest RMSE\nprint(\"Best parameters found: \", grid_mse_rf.best_params_)\n\nprint(\"Lowest R squared found: \", np.sqrt(np.abs(grid_mse_rf.best_score_)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow, contrary to my earlier assumptions; hyperparameter tuning improved the random forest performance by a massive 10%"},{"metadata":{"trusted":true},"cell_type":"code","source":"# let us predict the results with the new random forest regressor and see how it performs in kaggle\n\npred_rf = grid_mse_rf.predict(df_test)\n\n# let us put this into the submission format\n\nrf_output=pd.DataFrame({'Id':df_test.index, 'SalePrice': pred_rf}) \n\n# save to kaggle\n\nrf_output.to_csv('my_submission_rf_withhyperparametertuning.csv', index=False)\n\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameter Tuning using Informed Search - Bayesian Optimization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# hyperparameter tuning is very important for models like XGBoost\n\n# while random search cv gives us a good chance of finding optimal parameters, let us try a newer method of informed search\n\n# Bayesian Optimization is what we will try\n\n# detailed e.g. in datacamp and in https://towardsdatascience.com/an-introductory-example-of-bayesian-optimization-in-python-with-hyperopt-aae40fff4ff0\n\n# let us import the hyperplot package which will do this for us\n\nfrom hyperopt import fmin, tpe, hp\n\nimport hyperopt.pyll.stochastic as st\n\n# first step is to build the grid or domain\n\nspace = {'regressor__learning_rate': hp.uniform('regressor__learning_rate',0.05,1.05),'regressor__n_estimators': hp.quniform('regressor__n_estimators',500,2000,100),'regressor__subsample': hp.uniform('regressor__subsample',0.05,1.05),'regressor__max_depth' : hp.quniform('regressor__max_depth', 2,6,1),'regressor__colsample_bytree': hp.uniform('regressor__colsample_bytree',0.05,1.05),}\n\n# next, define the objective function\n\ndef objective(params):\n    \n params = {'regressor__learning_rate': params['regressor__learning_rate'],'regressor__n_estimators': int(params['regressor__n_estimators']), 'regressor__subsample': params['regressor__subsample'],'regressor__max_depth': int(params['regressor__max_depth']), 'regressor__colsample_bytree': params['regressor__colsample_bytree']}\n \n import xgboost as xgb\n\n xgb1= xgb.XGBRegressor(seed=123)\n\n xgb_pipe = Pipeline(steps=[('preprocessor', preprocessor),('regressor',  xgb1)])\n\n best_score = cross_val_score(xgb_pipe, X, y,cv=10, n_jobs=-1).mean()\n loss = 1 - best_score\n \n return loss\n\nprint (st.sample(space))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now let us run the optimization algorithm\n\nbest_result = fmin(fn=objective,space=space,max_evals=25,rstate=np.random.RandomState(42),algo=tpe.suggest)\n\nbest_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let us run the XgBoost algorim with the best_result hyperparameters\n\n# import random search cv\n\nimport xgboost as xgb\n\nxgb= xgb.XGBRegressor(seed=123,colsample_bytree=0.5351871929759634,learning_rate = 0.36944725103976733,max_depth=2,n_estimators=1700, subsample=0.50 )\n\nxgb_pipe_bayes= Pipeline(steps=[('preprocessor', preprocessor),('regressor',  xgb)])\n\n# Fit the pipeline object to the data\n\nxgb_pipe_bayes.fit(X_train, np.ravel(y_train))\n\n# Predict the labels of the test set\n\ny_pred = xgb_pipe_bayes.predict(X_test)\n\n# Evaluate accuracies using cross_val_score\n\ncv_scores_r2 = cross_val_score(xgb_pipe_bayes,X,np.ravel(y),cv=5).mean()\n\n# Print the best parameters and lowest RMSE\n\nprint(\"Lowest R squared found: \", cv_scores_r2)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not great, the bayer hyper parameter optimization is giving us a worse off result than even if we do not set any hyper parameters"},{"metadata":{},"cell_type":"markdown","source":"# TPOT algorithm run \n\nLet's now turn ourselves to our ML assistant TPOT"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Handy guide to TPOT is in DataCamp Hyper parameter course as well as in https://towardsdatascience.com/tpot-automated-machine-learning-in-python-4c063b3e5de9\n\n# TPOT claims to do all pre-processing of data\n\n# let us pick the raw housing data in that case without any pre-processing\n\n# df_train holds our training data\n\n# tpot seems to run into problems with one hot encoding in this dataset\n\n# let us then use our own pre-processing steps as in pipeline, comment out everything else\n\n#df_train.head()\n\n#X = df_train.drop(columns=['SalePrice'])\n\n#y = df_train['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let us import train test split\n\n#from sklearn.model_selection import train_test_split\n\n# Split data into 70% train and 30% test\n\n#X_train, X_test, y_train, y_test = train_test_split(X, y,test_size= 0.3,random_state= 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let us now try tpot\n\n# remember to not use TPOT default settings as that may run for well over an hour \n\nfrom tpot import TPOTRegressor\n\ntpot = TPOTRegressor(generations=20, population_size=20,verbosity=2, offspring_size=20,config_dict='TPOT sparse',scoring='r2', cv=5,n_jobs=-1)\n\ntpot_pipe = Pipeline(steps=[('preprocessor', preprocessor),('regressor',  tpot)])\n\ntpot_pipe.fit(X_train, np.ravel(y_train))\n\ntpot_pipe.score(X_test, np.ravel(y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Does evaluate that an XGBoost Regressor is the best! But best result is 89% given that the default XGBoost gave 88% accuracy, this is a disappointment. Should really be running it for much more iterations for TPOT to be really useful "},{"metadata":{"trusted":true},"cell_type":"code","source":"# let us use TPOT's export function to export the code of the best pipeline\n\n# acess tpot in pipeline\n\ntpot_step = tpot_pipe[-1]\n\ntpot_step.export('tpt.py')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tpot_step.evaluated_individuals_","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}